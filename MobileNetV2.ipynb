{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ykAZa7jcxm7c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Conv2D,MaxPool2D,concatenate,AveragePooling2D,Add,GlobalAveragePooling2D,Flatten,ZeroPadding2D,BatchNormalization,Dense,ZeroPadding2D,Activation,ReLU,Input,DepthwiseConv2D\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from keras.preprocessing import image\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU') \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5xwBZf8SJn_8"
   },
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "train_dir=r'F:\\JUPYTER NOTEBOOK\\GROUNDNUT\\dataset\\Train'\n",
    "test_dir=r'F:\\JUPYTER NOTEBOOK\\GROUNDNUT\\dataset\\Test'\n",
    "valid_dir=r'F:\\JUPYTER NOTEBOOK\\GROUNDNUT\\dataset\\Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CvuDm_pgJn_9"
   },
   "outputs": [],
   "source": [
    "#Rescaling and augmentation of data\n",
    "data_augmentation=tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\",input_shape=(224,224,3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "],name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WW8Ztgk-Jn__",
    "outputId": "920e5fa8-0d85-498e-8ab0-b24ca2fb8163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE=(224,224)\n",
    "BATCH_SIZE=5\n",
    "training_set=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = test_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode = 'categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = valid_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode = 'categorical',\n",
    ")\n",
    "class_names=validation_set.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U6XfIjOkNnUC"
   },
   "outputs": [],
   "source": [
    "def inverted_residual_block(inputs, filters, expansion, stride, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Constructs an inverted residual block with bottleneck.\n",
    "\n",
    "    Arguments:\n",
    "    - inputs: Input tensor.\n",
    "    - filters: Number of filters for the pointwise convolution.\n",
    "    - expansion: Expansion factor for the inner bottleneck.\n",
    "    - stride: Stride of the depthwise convolution.\n",
    "    - kernel_size: Kernel size of the depthwise convolution.\n",
    "\n",
    "    Returns:\n",
    "    - Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pointwise convolution expanding dimensions\n",
    "    x = Conv2D(filters * expansion, (1, 1), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Depthwise convolution\n",
    "    x = DepthwiseConv2D(kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Pointwise convolution squeezing back dimensions\n",
    "    x = Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Adding the residual connection when possible\n",
    "    if stride == 1 and inputs.shape[-1] == filters:\n",
    "        x = Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "def MobileNetV2(input_shape=(224, 224, 3), classes=5):\n",
    "    \"\"\"\n",
    "    Constructs the MobileNetV2 architecture.\n",
    "\n",
    "    Arguments:\n",
    "    - input_shape: Shape of the input tensor.\n",
    "    - num_classes: Number of classes for the classification task.\n",
    "\n",
    "    Returns:\n",
    "    - Model object.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial convolution layer with larger filter size and stride\n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Sequence of inverted residual blocks\n",
    "    # The configurations of filters, expansion factors, strides, and kernel sizes\n",
    "    # are based on the MobileNetV2 paper and its typical implementations\n",
    "    x = inverted_residual_block(x, 16, 1, 1, 3)\n",
    "    x = inverted_residual_block(x, 24, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 24, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 32, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 32, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 32, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 64, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 64, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 64, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 64, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 96, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 96, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 96, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 160, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 160, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 160, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 320, 6, 1, 3)\n",
    "\n",
    "    # Final pointwise convolution layer\n",
    "    x = Conv2D(1280, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Global pooling and fully connected layer for classification\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=inputs,outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y1h9OqVWkxvf"
   },
   "outputs": [],
   "source": [
    "model=MobileNetV2(input_shape=(224,224,3),classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kt6IBEy1JoAJ"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "#Checkpoint to save the best model per epoch\n",
    "model_path=r'C:\\Users\\ani\\Desktop\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
    "checkpoint=ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1FPPpQFkJoAL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 32  896         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 112, 112, 32  128        ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 112, 112, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 112, 112, 16  528         ['re_lu[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 112, 112, 16  64         ['conv2d_1[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 112, 112, 16  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 112, 112, 16  160        ['re_lu_1[0][0]']                \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 112, 112, 16  64         ['depthwise_conv2d[0][0]']       \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 112, 112, 16  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 112, 112, 16  272         ['re_lu_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 112, 112, 16  64         ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 112, 112, 14  2448        ['batch_normalization_3[0][0]']  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 112, 112, 14  576        ['conv2d_3[0][0]']               \n",
      " rmalization)                   4)                                                                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 112, 112, 14  0           ['batch_normalization_4[0][0]']  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " depthwise_conv2d_1 (DepthwiseC  (None, 56, 56, 144)  1440       ['re_lu_3[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 56, 56, 144)  576        ['depthwise_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 56, 56, 144)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 56, 56, 24)   3480        ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 56, 56, 24)  96          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 56, 56, 144)  3600        ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 56, 56, 144)  576        ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 56, 56, 144)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " depthwise_conv2d_2 (DepthwiseC  (None, 56, 56, 144)  1440       ['re_lu_5[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 56, 56, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 56, 56, 144)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 56, 56, 24)   3480        ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 56, 56, 24)  96          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 56, 56, 24)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'batch_normalization_6[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 56, 56, 192)  4800        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 56, 56, 192)  768        ['conv2d_7[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 56, 56, 192)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_3 (DepthwiseC  (None, 28, 28, 192)  1920       ['re_lu_7[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 28, 28, 192)  768        ['depthwise_conv2d_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 28, 28, 192)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 28, 28, 32)   6176        ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 28, 28, 32)  128         ['conv2d_8[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 28, 28, 192)  6336        ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 28, 28, 192)  768        ['conv2d_9[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 28, 28, 192)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_4 (DepthwiseC  (None, 28, 28, 192)  1920       ['re_lu_9[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 28, 28, 192)  768        ['depthwise_conv2d_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 28, 28, 192)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 28, 28, 32)   6176        ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 28, 28, 32)  128         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 28, 28, 32)   0           ['batch_normalization_15[0][0]', \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 28, 28, 192)  6336        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 28, 28, 192)  768        ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 28, 28, 192)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_5 (DepthwiseC  (None, 28, 28, 192)  1920       ['re_lu_11[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 28, 28, 192)  768        ['depthwise_conv2d_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 28, 28, 192)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 28, 28, 32)   6176        ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 28, 28, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 28, 28, 32)   0           ['batch_normalization_18[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 28, 28, 384)  12672       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 28, 28, 384)  1536       ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 28, 28, 384)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_6 (DepthwiseC  (None, 14, 14, 384)  3840       ['re_lu_13[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_14[0][0]']               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 14, 14, 64)  256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 14, 14, 384)  24960       ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 14, 14, 384)  1536       ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_7 (DepthwiseC  (None, 14, 14, 384)  3840       ['re_lu_15[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 14, 14, 64)  256         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 14, 14, 64)   0           ['batch_normalization_24[0][0]', \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 14, 14, 384)  24960       ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 14, 14, 384)  1536       ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_8 (DepthwiseC  (None, 14, 14, 384)  9984       ['re_lu_17[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_8[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 14, 14, 64)  256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 14, 14, 64)   0           ['batch_normalization_27[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 14, 14, 384)  24960       ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 14, 14, 384)  1536       ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_9 (DepthwiseC  (None, 14, 14, 384)  9984       ['re_lu_19[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_9[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 14, 14, 64)  256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 14, 14, 64)   0           ['batch_normalization_30[0][0]', \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 14, 14, 576)  37440       ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 14, 14, 576)  2304       ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_10 (Depthwise  (None, 14, 14, 576)  14976      ['re_lu_21[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_32 (BatchN  (None, 14, 14, 576)  2304       ['depthwise_conv2d_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 14, 14, 96)   55392       ['re_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 14, 14, 96)  384         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 14, 14, 576)  55872       ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 14, 14, 576)  2304       ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_11 (Depthwise  (None, 14, 14, 576)  5760       ['re_lu_23[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 14, 14, 576)  2304       ['depthwise_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 14, 14, 96)   55392       ['re_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 14, 14, 96)  384         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 14, 14, 96)   0           ['batch_normalization_36[0][0]', \n",
      "                                                                  'batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 14, 14, 576)  55872       ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 14, 14, 576)  2304       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_12 (Depthwise  (None, 14, 14, 576)  5760       ['re_lu_25[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 14, 14, 576)  2304       ['depthwise_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 14, 14, 96)   55392       ['re_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 14, 14, 96)  384         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 14, 14, 96)   0           ['batch_normalization_39[0][0]', \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 14, 14, 960)  93120       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 14, 14, 960)  3840       ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 14, 14, 960)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_13 (Depthwise  (None, 7, 7, 960)   9600        ['re_lu_27[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 7, 7, 960)   3840        ['depthwise_conv2d_13[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 7, 7, 160)    153760      ['re_lu_28[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 7, 7, 160)   640         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 7, 7, 960)    154560      ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 7, 7, 960)   3840        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " depthwise_conv2d_14 (Depthwise  (None, 7, 7, 960)   24960       ['re_lu_29[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 7, 7, 960)   3840        ['depthwise_conv2d_14[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 7, 7, 160)    153760      ['re_lu_30[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 7, 7, 160)   640         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 7, 7, 160)    0           ['batch_normalization_45[0][0]', \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 7, 7, 960)    154560      ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 7, 7, 960)   3840        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_15 (Depthwise  (None, 7, 7, 960)   24960       ['re_lu_31[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 7, 7, 960)   3840        ['depthwise_conv2d_15[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 7, 7, 160)    153760      ['re_lu_32[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 7, 7, 160)   640         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 7, 7, 160)    0           ['batch_normalization_48[0][0]', \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 7, 7, 1920)   309120      ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 7, 7, 1920)  7680        ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)                (None, 7, 7, 1920)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_16 (Depthwise  (None, 7, 7, 1920)  19200       ['re_lu_33[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 7, 7, 1920)  7680        ['depthwise_conv2d_16[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)                (None, 7, 7, 1920)   0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 7, 7, 320)    614720      ['re_lu_34[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 7, 7, 320)   1280        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 7, 7, 1280)   410880      ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 7, 7, 1280)  5120        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)                (None, 7, 7, 1280)   0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['re_lu_35[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            6405        ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,981,301\n",
      "Trainable params: 2,939,893\n",
      "Non-trainable params: 41,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "87_dyg84JoAN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HarG5-LZJoAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.3019 - accuracy: 0.4847\n",
      "Epoch 1: val_loss improved from inf to 1.67381, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet01-0.3488.hdf5\n",
      "276/276 [==============================] - 100s 253ms/step - loss: 1.3019 - accuracy: 0.4847 - val_loss: 1.6738 - val_accuracy: 0.3488\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.0053 - accuracy: 0.5858\n",
      "Epoch 2: val_loss did not improve from 1.67381\n",
      "276/276 [==============================] - 64s 228ms/step - loss: 1.0053 - accuracy: 0.5858 - val_loss: 2.0244 - val_accuracy: 0.3488\n",
      "Epoch 3/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.9319 - accuracy: 0.6364\n",
      "Epoch 3: val_loss did not improve from 1.67381\n",
      "276/276 [==============================] - 76s 273ms/step - loss: 0.9348 - accuracy: 0.6359 - val_loss: 2.4056 - val_accuracy: 0.4244\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.8676 - accuracy: 0.6541\n",
      "Epoch 4: val_loss improved from 1.67381 to 1.62540, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet04-0.5291.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.8676 - accuracy: 0.6541 - val_loss: 1.6254 - val_accuracy: 0.5291\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.6940\n",
      "Epoch 5: val_loss improved from 1.62540 to 1.62248, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet05-0.5988.hdf5\n",
      "276/276 [==============================] - 37s 131ms/step - loss: 0.7750 - accuracy: 0.6940 - val_loss: 1.6225 - val_accuracy: 0.5988\n",
      "Epoch 6/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.6649 - accuracy: 0.7476\n",
      "Epoch 6: val_loss did not improve from 1.62248\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.6652 - accuracy: 0.7471 - val_loss: 2.0331 - val_accuracy: 0.4709\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6272 - accuracy: 0.7529\n",
      "Epoch 7: val_loss did not improve from 1.62248\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.6272 - accuracy: 0.7529 - val_loss: 2.6206 - val_accuracy: 0.5407\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.7783\n",
      "Epoch 8: val_loss did not improve from 1.62248\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.6093 - accuracy: 0.7783 - val_loss: 2.0602 - val_accuracy: 0.5581\n",
      "Epoch 9/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.5446 - accuracy: 0.8022\n",
      "Epoch 9: val_loss improved from 1.62248 to 1.39754, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet09-0.6395.hdf5\n",
      "276/276 [==============================] - 37s 131ms/step - loss: 0.5482 - accuracy: 0.8016 - val_loss: 1.3975 - val_accuracy: 0.6395\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.8140\n",
      "Epoch 10: val_loss improved from 1.39754 to 1.26425, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet10-0.6744.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.5112 - accuracy: 0.8140 - val_loss: 1.2643 - val_accuracy: 0.6744\n",
      "Epoch 11/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.8225\n",
      "Epoch 11: val_loss improved from 1.26425 to 0.84148, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet11-0.7035.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.4849 - accuracy: 0.8219 - val_loss: 0.8415 - val_accuracy: 0.7035\n",
      "Epoch 12/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8436\n",
      "Epoch 12: val_loss improved from 0.84148 to 0.57306, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet12-0.8023.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.4227 - accuracy: 0.8430 - val_loss: 0.5731 - val_accuracy: 0.8023\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.8408\n",
      "Epoch 13: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.4619 - accuracy: 0.8408 - val_loss: 2.8020 - val_accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.8285\n",
      "Epoch 14: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.4739 - accuracy: 0.8285 - val_loss: 0.6635 - val_accuracy: 0.7558\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8459\n",
      "Epoch 15: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.4303 - accuracy: 0.8459 - val_loss: 0.6317 - val_accuracy: 0.8198\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.8983\n",
      "Epoch 16: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.2772 - accuracy: 0.8983 - val_loss: 1.9411 - val_accuracy: 0.7209\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.8706\n",
      "Epoch 17: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.3434 - accuracy: 0.8706 - val_loss: 0.6245 - val_accuracy: 0.8314\n",
      "Epoch 18/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.3601 - accuracy: 0.8771\n",
      "Epoch 18: val_loss improved from 0.57306 to 0.45022, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet18-0.8779.hdf5\n",
      "276/276 [==============================] - 37s 132ms/step - loss: 0.3602 - accuracy: 0.8772 - val_loss: 0.4502 - val_accuracy: 0.8779\n",
      "Epoch 19/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8938\n",
      "Epoch 19: val_loss did not improve from 0.45022\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.3015 - accuracy: 0.8932 - val_loss: 0.4549 - val_accuracy: 0.8023\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.9041\n",
      "Epoch 20: val_loss improved from 0.45022 to 0.44926, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet20-0.8895.hdf5\n",
      "276/276 [==============================] - 37s 130ms/step - loss: 0.2799 - accuracy: 0.9041 - val_loss: 0.4493 - val_accuracy: 0.8895\n",
      "Epoch 21/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.9178\n",
      "Epoch 21: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.2503 - accuracy: 0.9179 - val_loss: 0.9803 - val_accuracy: 0.7616\n",
      "Epoch 22/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9135\n",
      "Epoch 22: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.2665 - accuracy: 0.9135 - val_loss: 0.8077 - val_accuracy: 0.8081\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9346\n",
      "Epoch 23: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1990 - accuracy: 0.9346 - val_loss: 0.5037 - val_accuracy: 0.8837\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.9186\n",
      "Epoch 24: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.2612 - accuracy: 0.9186 - val_loss: 0.9238 - val_accuracy: 0.7733\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.8983\n",
      "Epoch 25: val_loss improved from 0.44926 to 0.42447, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet25-0.8663.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.2796 - accuracy: 0.8983 - val_loss: 0.4245 - val_accuracy: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9331\n",
      "Epoch 26: val_loss improved from 0.42447 to 0.39421, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet26-0.9070.hdf5\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.1957 - accuracy: 0.9331 - val_loss: 0.3942 - val_accuracy: 0.9070\n",
      "Epoch 27/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9375\n",
      "Epoch 27: val_loss did not improve from 0.39421\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.2005 - accuracy: 0.9375 - val_loss: 1.6773 - val_accuracy: 0.6453\n",
      "Epoch 28/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9680\n",
      "Epoch 28: val_loss improved from 0.39421 to 0.38159, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet28-0.8953.hdf5\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.1059 - accuracy: 0.9680 - val_loss: 0.3816 - val_accuracy: 0.8953\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9484\n",
      "Epoch 29: val_loss did not improve from 0.38159\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1765 - accuracy: 0.9484 - val_loss: 0.4797 - val_accuracy: 0.8779\n",
      "Epoch 30/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1579 - accuracy: 0.9491\n",
      "Epoch 30: val_loss improved from 0.38159 to 0.34691, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet30-0.9128.hdf5\n",
      "276/276 [==============================] - 36s 128ms/step - loss: 0.1589 - accuracy: 0.9484 - val_loss: 0.3469 - val_accuracy: 0.9128\n",
      "Epoch 31/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9636\n",
      "Epoch 31: val_loss improved from 0.34691 to 0.17775, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet31-0.9593.hdf5\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.1247 - accuracy: 0.9637 - val_loss: 0.1778 - val_accuracy: 0.9593\n",
      "Epoch 32/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9607\n",
      "Epoch 32: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.1147 - accuracy: 0.9608 - val_loss: 0.8016 - val_accuracy: 0.8081\n",
      "Epoch 33/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9571\n",
      "Epoch 33: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1615 - accuracy: 0.9571 - val_loss: 0.8278 - val_accuracy: 0.8372\n",
      "Epoch 34/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9397\n",
      "Epoch 34: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1656 - accuracy: 0.9397 - val_loss: 0.5951 - val_accuracy: 0.8779\n",
      "Epoch 35/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9585\n",
      "Epoch 35: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1215 - accuracy: 0.9578 - val_loss: 0.6440 - val_accuracy: 0.8547\n",
      "Epoch 36/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9745\n",
      "Epoch 36: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0872 - accuracy: 0.9746 - val_loss: 1.2387 - val_accuracy: 0.8081\n",
      "Epoch 37/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9578\n",
      "Epoch 37: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1275 - accuracy: 0.9578 - val_loss: 0.4122 - val_accuracy: 0.9012\n",
      "Epoch 38/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1533 - accuracy: 0.9469\n",
      "Epoch 38: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1569 - accuracy: 0.9462 - val_loss: 0.7631 - val_accuracy: 0.7267\n",
      "Epoch 39/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9680\n",
      "Epoch 39: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0987 - accuracy: 0.9680 - val_loss: 0.2289 - val_accuracy: 0.9419\n",
      "Epoch 40/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9869\n",
      "Epoch 40: val_loss improved from 0.17775 to 0.14820, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet40-0.9709.hdf5\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0444 - accuracy: 0.9869 - val_loss: 0.1482 - val_accuracy: 0.9709\n",
      "Epoch 41/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9469\n",
      "Epoch 41: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.1840 - accuracy: 0.9469 - val_loss: 1.2799 - val_accuracy: 0.7442\n",
      "Epoch 42/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9695\n",
      "Epoch 42: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0947 - accuracy: 0.9695 - val_loss: 0.2589 - val_accuracy: 0.9128\n",
      "Epoch 43/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9724\n",
      "Epoch 43: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0964 - accuracy: 0.9724 - val_loss: 0.2284 - val_accuracy: 0.9419\n",
      "Epoch 44/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.9731\n",
      "Epoch 44: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0813 - accuracy: 0.9731 - val_loss: 0.4869 - val_accuracy: 0.8895\n",
      "Epoch 45/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9644\n",
      "Epoch 45: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1065 - accuracy: 0.9644 - val_loss: 0.3298 - val_accuracy: 0.9244\n",
      "Epoch 46/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9673\n",
      "Epoch 46: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1097 - accuracy: 0.9673 - val_loss: 0.2838 - val_accuracy: 0.9360\n",
      "Epoch 47/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9782\n",
      "Epoch 47: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0682 - accuracy: 0.9782 - val_loss: 0.5353 - val_accuracy: 0.8779\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9833\n",
      "Epoch 48: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.5576 - val_accuracy: 0.8430\n",
      "Epoch 49/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9600\n",
      "Epoch 49: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1358 - accuracy: 0.9600 - val_loss: 0.3484 - val_accuracy: 0.8895\n",
      "Epoch 50/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9782\n",
      "Epoch 50: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0782 - accuracy: 0.9782 - val_loss: 0.2650 - val_accuracy: 0.9244\n",
      "Epoch 51/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.9825\n",
      "Epoch 51: val_loss improved from 0.14820 to 0.13931, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet51-0.9651.hdf5\n",
      "276/276 [==============================] - 36s 128ms/step - loss: 0.0688 - accuracy: 0.9826 - val_loss: 0.1393 - val_accuracy: 0.9651\n",
      "Epoch 52/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9826\n",
      "Epoch 52: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0543 - accuracy: 0.9826 - val_loss: 0.3609 - val_accuracy: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9578\n",
      "Epoch 53: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1642 - accuracy: 0.9578 - val_loss: 0.5547 - val_accuracy: 0.8430\n",
      "Epoch 54/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9920\n",
      "Epoch 54: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0299 - accuracy: 0.9920 - val_loss: 0.2178 - val_accuracy: 0.9477\n",
      "Epoch 55/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9876\n",
      "Epoch 55: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0489 - accuracy: 0.9876 - val_loss: 0.2332 - val_accuracy: 0.9477\n",
      "Epoch 56/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9716\n",
      "Epoch 56: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0865 - accuracy: 0.9709 - val_loss: 0.5752 - val_accuracy: 0.9012\n",
      "Epoch 57/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9811\n",
      "Epoch 57: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0679 - accuracy: 0.9811 - val_loss: 0.2160 - val_accuracy: 0.9535\n",
      "Epoch 58/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 0.9876\n",
      "Epoch 58: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0429 - accuracy: 0.9876 - val_loss: 0.1776 - val_accuracy: 0.9593\n",
      "Epoch 59/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9796\n",
      "Epoch 59: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0786 - accuracy: 0.9797 - val_loss: 0.2215 - val_accuracy: 0.9477\n",
      "Epoch 60/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9927\n",
      "Epoch 60: val_loss improved from 0.13931 to 0.11801, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet60-0.9767.hdf5\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.1180 - val_accuracy: 0.9767\n",
      "Epoch 61/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9884\n",
      "Epoch 61: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0421 - accuracy: 0.9884 - val_loss: 0.3138 - val_accuracy: 0.9302\n",
      "Epoch 62/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0669 - accuracy: 0.9818\n",
      "Epoch 62: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0669 - accuracy: 0.9818 - val_loss: 0.2057 - val_accuracy: 0.9477\n",
      "Epoch 63/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9818\n",
      "Epoch 63: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0551 - accuracy: 0.9818 - val_loss: 0.5458 - val_accuracy: 0.9070\n",
      "Epoch 64/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9825\n",
      "Epoch 64: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0527 - accuracy: 0.9818 - val_loss: 0.9343 - val_accuracy: 0.8314\n",
      "Epoch 65/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9469\n",
      "Epoch 65: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1845 - accuracy: 0.9469 - val_loss: 0.1567 - val_accuracy: 0.9767\n",
      "Epoch 66/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0250 - accuracy: 0.9942\n",
      "Epoch 66: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0297 - accuracy: 0.9935 - val_loss: 0.2236 - val_accuracy: 0.9477\n",
      "Epoch 67/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0461 - accuracy: 0.9891\n",
      "Epoch 67: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0461 - accuracy: 0.9891 - val_loss: 0.1736 - val_accuracy: 0.9535\n",
      "Epoch 68/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 0.9869\n",
      "Epoch 68: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 35s 125ms/step - loss: 0.0460 - accuracy: 0.9869 - val_loss: 0.3459 - val_accuracy: 0.9477\n",
      "Epoch 69/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9905\n",
      "Epoch 69: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0278 - accuracy: 0.9898 - val_loss: 0.1710 - val_accuracy: 0.9651\n",
      "Epoch 70/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9862\n",
      "Epoch 70: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.4377 - val_accuracy: 0.9070\n",
      "Epoch 71/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9964\n",
      "Epoch 71: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0098 - accuracy: 0.9964 - val_loss: 0.4504 - val_accuracy: 0.9186\n",
      "Epoch 72/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9811\n",
      "Epoch 72: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0681 - accuracy: 0.9804 - val_loss: 0.8221 - val_accuracy: 0.8779\n",
      "Epoch 73/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9724\n",
      "Epoch 73: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0931 - accuracy: 0.9724 - val_loss: 1.0895 - val_accuracy: 0.6570\n",
      "Epoch 74/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9775\n",
      "Epoch 74: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0914 - accuracy: 0.9775 - val_loss: 0.2344 - val_accuracy: 0.9477\n",
      "Epoch 75/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 75: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2893 - val_accuracy: 0.9302\n",
      "Epoch 76/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9920\n",
      "Epoch 76: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.3401 - val_accuracy: 0.9302\n",
      "Epoch 77/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9971\n",
      "Epoch 77: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0161 - accuracy: 0.9971 - val_loss: 0.3257 - val_accuracy: 0.9186\n",
      "Epoch 78/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9782\n",
      "Epoch 78: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0659 - accuracy: 0.9782 - val_loss: 0.5459 - val_accuracy: 0.8953\n",
      "Epoch 79/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9789\n",
      "Epoch 79: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0650 - accuracy: 0.9789 - val_loss: 0.6055 - val_accuracy: 0.9070\n",
      "Epoch 80/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9956\n",
      "Epoch 80: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.3689 - val_accuracy: 0.9186\n",
      "Epoch 81/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9869\n",
      "Epoch 81: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0380 - accuracy: 0.9869 - val_loss: 0.5394 - val_accuracy: 0.8779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9869\n",
      "Epoch 82: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0501 - accuracy: 0.9869 - val_loss: 0.3339 - val_accuracy: 0.9360\n",
      "Epoch 83/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9709\n",
      "Epoch 83: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1050 - accuracy: 0.9709 - val_loss: 1.3468 - val_accuracy: 0.8081\n",
      "Epoch 84/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9767\n",
      "Epoch 84: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 128ms/step - loss: 0.0732 - accuracy: 0.9767 - val_loss: 0.2908 - val_accuracy: 0.9360\n",
      "Epoch 85/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9993\n",
      "Epoch 85: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0083 - accuracy: 0.9993 - val_loss: 0.2599 - val_accuracy: 0.9477\n",
      "Epoch 86/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 86: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2244 - val_accuracy: 0.9651\n",
      "Epoch 87/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9985\n",
      "Epoch 87: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.2510 - val_accuracy: 0.9477\n",
      "Epoch 88/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9927\n",
      "Epoch 88: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.5407 - val_accuracy: 0.8895\n",
      "Epoch 89/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9556\n",
      "Epoch 89: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1545 - accuracy: 0.9557 - val_loss: 0.4296 - val_accuracy: 0.9244\n",
      "Epoch 90/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9949\n",
      "Epoch 90: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0232 - accuracy: 0.9942 - val_loss: 0.3798 - val_accuracy: 0.9128\n",
      "Epoch 91/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9775\n",
      "Epoch 91: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0846 - accuracy: 0.9775 - val_loss: 0.1992 - val_accuracy: 0.9419\n",
      "Epoch 92/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9920\n",
      "Epoch 92: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0182 - accuracy: 0.9920 - val_loss: 0.2639 - val_accuracy: 0.9419\n",
      "Epoch 93/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9971\n",
      "Epoch 93: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.3010 - val_accuracy: 0.9477\n",
      "Epoch 94/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 94: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.1581 - val_accuracy: 0.9593\n",
      "Epoch 95/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0326 - accuracy: 0.9913\n",
      "Epoch 95: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 0.2131 - val_accuracy: 0.9651\n",
      "Epoch 96/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9964\n",
      "Epoch 96: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.2882 - val_accuracy: 0.9302\n",
      "Epoch 97/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9985\n",
      "Epoch 97: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.2528 - val_accuracy: 0.9419\n",
      "Epoch 98/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 98: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.2174 - val_accuracy: 0.9593\n",
      "Epoch 99/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9985\n",
      "Epoch 99: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.1925 - val_accuracy: 0.9651\n",
      "Epoch 100/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9753\n",
      "Epoch 100: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0998 - accuracy: 0.9753 - val_loss: 0.2133 - val_accuracy: 0.9244\n",
      "Epoch 101/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9797\n",
      "Epoch 101: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0625 - accuracy: 0.9797 - val_loss: 0.3882 - val_accuracy: 0.9302\n",
      "Epoch 102/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9971\n",
      "Epoch 102: val_loss improved from 0.11801 to 0.10895, saving model to C:\\Users\\ani\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet102-0.9767.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.0191 - accuracy: 0.9971 - val_loss: 0.1090 - val_accuracy: 0.9767\n",
      "Epoch 103/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9942\n",
      "Epoch 103: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0201 - accuracy: 0.9942 - val_loss: 0.3299 - val_accuracy: 0.9302\n",
      "Epoch 104/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9876\n",
      "Epoch 104: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0459 - accuracy: 0.9876 - val_loss: 0.2496 - val_accuracy: 0.9360\n",
      "Epoch 105/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0510 - accuracy: 0.9825\n",
      "Epoch 105: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0510 - accuracy: 0.9826 - val_loss: 0.4094 - val_accuracy: 0.8953\n",
      "Epoch 106/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 106: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.1640 - val_accuracy: 0.9651\n",
      "Epoch 107/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9833\n",
      "Epoch 107: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.1492 - val_accuracy: 0.9767\n",
      "Epoch 108/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9978\n",
      "Epoch 108: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.1733 - val_accuracy: 0.9826\n",
      "Epoch 109/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9985\n",
      "Epoch 109: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.2468 - val_accuracy: 0.9477\n",
      "Epoch 110/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9978\n",
      "Epoch 110: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0049 - accuracy: 0.9978 - val_loss: 0.1902 - val_accuracy: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9985\n",
      "Epoch 111: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.1364 - val_accuracy: 0.9709\n",
      "Epoch 112/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9782\n",
      "Epoch 112: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0872 - accuracy: 0.9782 - val_loss: 0.4604 - val_accuracy: 0.9244\n",
      "Epoch 113/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9796\n",
      "Epoch 113: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0642 - accuracy: 0.9797 - val_loss: 0.3254 - val_accuracy: 0.9186\n",
      "Epoch 114/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9978\n",
      "Epoch 114: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.1367 - val_accuracy: 0.9651\n",
      "Epoch 115/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9898\n",
      "Epoch 115: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0278 - accuracy: 0.9898 - val_loss: 0.1864 - val_accuracy: 0.9477\n",
      "Epoch 116/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9891\n",
      "Epoch 116: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0316 - accuracy: 0.9891 - val_loss: 0.3381 - val_accuracy: 0.9419\n",
      "Epoch 117/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9964\n",
      "Epoch 117: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0186 - accuracy: 0.9964 - val_loss: 0.1816 - val_accuracy: 0.9535\n",
      "Epoch 118/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9906\n",
      "Epoch 118: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0211 - accuracy: 0.9906 - val_loss: 0.1995 - val_accuracy: 0.9419\n",
      "Epoch 119/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9964\n",
      "Epoch 119: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0162 - accuracy: 0.9964 - val_loss: 0.3037 - val_accuracy: 0.9186\n",
      "Epoch 120/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9978\n",
      "Epoch 120: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0152 - accuracy: 0.9978 - val_loss: 0.1522 - val_accuracy: 0.9535\n",
      "Epoch 121/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9906\n",
      "Epoch 121: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.1263 - val_accuracy: 0.9709\n",
      "Epoch 122/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9956\n",
      "Epoch 122: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0150 - accuracy: 0.9956 - val_loss: 0.3205 - val_accuracy: 0.9302\n",
      "Epoch 123/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9985\n",
      "Epoch 123: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.1968 - val_accuracy: 0.9535\n",
      "Epoch 124/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9869\n",
      "Epoch 124: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0380 - accuracy: 0.9869 - val_loss: 0.8270 - val_accuracy: 0.8605\n",
      "Epoch 125/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9876\n",
      "Epoch 125: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0320 - accuracy: 0.9876 - val_loss: 0.2471 - val_accuracy: 0.9360\n",
      "Epoch 126/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9775\n",
      "Epoch 126: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0903 - accuracy: 0.9775 - val_loss: 0.2147 - val_accuracy: 0.9419\n",
      "Epoch 127/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9956\n",
      "Epoch 127: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0126 - accuracy: 0.9956 - val_loss: 0.1897 - val_accuracy: 0.9767\n",
      "Epoch 128/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 128: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.1273 - val_accuracy: 0.9826\n",
      "Epoch 129/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9978\n",
      "Epoch 129: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0051 - accuracy: 0.9978 - val_loss: 0.1763 - val_accuracy: 0.9709\n",
      "Epoch 130/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9956\n",
      "Epoch 130: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0095 - accuracy: 0.9956 - val_loss: 0.2668 - val_accuracy: 0.9302\n",
      "Epoch 131/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9949\n",
      "Epoch 131: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0119 - accuracy: 0.9949 - val_loss: 0.1332 - val_accuracy: 0.9826\n",
      "Epoch 132/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 132: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9651\n",
      "Epoch 133/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 133: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1913 - val_accuracy: 0.9593\n",
      "Epoch 134/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 5.8337e-04 - accuracy: 1.0000\n",
      "Epoch 134: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 5.8337e-04 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9593\n",
      "Epoch 135/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 135: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1741 - val_accuracy: 0.9709\n",
      "Epoch 136/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9797\n",
      "Epoch 136: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0801 - accuracy: 0.9797 - val_loss: 1.9446 - val_accuracy: 0.7326\n",
      "Epoch 137/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0430 - accuracy: 0.9891\n",
      "Epoch 137: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0431 - accuracy: 0.9891 - val_loss: 0.2375 - val_accuracy: 0.9593\n",
      "Epoch 138/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9949\n",
      "Epoch 138: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.2606 - val_accuracy: 0.9419\n",
      "Epoch 139/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9891\n",
      "Epoch 139: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0319 - accuracy: 0.9891 - val_loss: 1.5143 - val_accuracy: 0.7791\n",
      "Epoch 140/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9942\n",
      "Epoch 140: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.1158 - val_accuracy: 0.9709\n",
      "Epoch 141/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 141: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1370 - val_accuracy: 0.9709\n",
      "Epoch 142/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9985\n",
      "Epoch 142: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 125ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.1387 - val_accuracy: 0.9593\n",
      "Epoch 143/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0390 - accuracy: 0.9891\n",
      "Epoch 143: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0390 - accuracy: 0.9891 - val_loss: 0.1111 - val_accuracy: 0.9709\n",
      "Epoch 144/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9775\n",
      "Epoch 144: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0736 - accuracy: 0.9775 - val_loss: 0.2438 - val_accuracy: 0.9302\n",
      "Epoch 145/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9935\n",
      "Epoch 145: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0175 - accuracy: 0.9927 - val_loss: 0.1398 - val_accuracy: 0.9651\n",
      "Epoch 146/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9935\n",
      "Epoch 146: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0331 - accuracy: 0.9935 - val_loss: 0.2104 - val_accuracy: 0.9535\n",
      "Epoch 147/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9942\n",
      "Epoch 147: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0175 - accuracy: 0.9942 - val_loss: 0.3549 - val_accuracy: 0.8953\n",
      "Epoch 148/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9993\n",
      "Epoch 148: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0104 - accuracy: 0.9993 - val_loss: 0.2482 - val_accuracy: 0.9302\n",
      "Epoch 149/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9920\n",
      "Epoch 149: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0210 - accuracy: 0.9920 - val_loss: 0.2592 - val_accuracy: 0.9593\n",
      "Epoch 150/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9985\n",
      "Epoch 150: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.3430 - val_accuracy: 0.9360\n",
      "Epoch 151/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 7.5866e-04 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 7.5866e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9477\n",
      "Epoch 152/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.9302\n",
      "Epoch 153/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9971\n",
      "Epoch 153: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0074 - accuracy: 0.9971 - val_loss: 0.2211 - val_accuracy: 0.9419\n",
      "Epoch 154/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2544 - val_accuracy: 0.9360\n",
      "Epoch 155/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9862\n",
      "Epoch 155: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0363 - accuracy: 0.9862 - val_loss: 1.3961 - val_accuracy: 0.7209\n",
      "Epoch 156/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9818\n",
      "Epoch 156: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0636 - accuracy: 0.9818 - val_loss: 0.5941 - val_accuracy: 0.8779\n",
      "Epoch 157/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9855\n",
      "Epoch 157: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0433 - accuracy: 0.9855 - val_loss: 0.2356 - val_accuracy: 0.9535\n",
      "Epoch 158/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0300 - accuracy: 0.9898\n",
      "Epoch 158: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0300 - accuracy: 0.9898 - val_loss: 0.1326 - val_accuracy: 0.9593\n",
      "Epoch 159/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9956\n",
      "Epoch 159: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0200 - accuracy: 0.9956 - val_loss: 0.1092 - val_accuracy: 0.9651\n",
      "Epoch 160/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9964\n",
      "Epoch 160: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0157 - accuracy: 0.9964 - val_loss: 0.1686 - val_accuracy: 0.9477\n",
      "Epoch 161/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9971\n",
      "Epoch 161: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0086 - accuracy: 0.9964 - val_loss: 0.2389 - val_accuracy: 0.9535\n",
      "Epoch 162/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9847\n",
      "Epoch 162: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0575 - accuracy: 0.9847 - val_loss: 0.1721 - val_accuracy: 0.9477\n",
      "Epoch 163/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 163: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0081 - accuracy: 0.9971 - val_loss: 0.2584 - val_accuracy: 0.9477\n",
      "Epoch 164/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9985\n",
      "Epoch 164: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0032 - accuracy: 0.9985 - val_loss: 0.2131 - val_accuracy: 0.9593\n",
      "Epoch 165/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 5.9109e-04 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 5.9117e-04 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9593\n",
      "Epoch 166/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2243 - val_accuracy: 0.9651\n",
      "Epoch 167/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9993\n",
      "Epoch 167: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.2005 - val_accuracy: 0.9593\n",
      "Epoch 168/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0499 - accuracy: 0.9825\n",
      "Epoch 168: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0499 - accuracy: 0.9826 - val_loss: 0.4842 - val_accuracy: 0.8895\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9956\n",
      "Epoch 169: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0106 - accuracy: 0.9956 - val_loss: 0.4059 - val_accuracy: 0.9244\n",
      "Epoch 170/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9985\n",
      "Epoch 170: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.1164 - val_accuracy: 0.9593\n",
      "Epoch 171/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9942\n",
      "Epoch 171: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0244 - accuracy: 0.9942 - val_loss: 0.2145 - val_accuracy: 0.9419\n",
      "Epoch 172/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9971\n",
      "Epoch 172: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.1857 - val_accuracy: 0.9535\n",
      "Epoch 173/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9927\n",
      "Epoch 173: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0251 - accuracy: 0.9927 - val_loss: 0.9524 - val_accuracy: 0.8953\n",
      "Epoch 174/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 174: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.1697 - val_accuracy: 0.9651\n",
      "Epoch 175/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9993\n",
      "Epoch 175: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0088 - accuracy: 0.9993 - val_loss: 0.1287 - val_accuracy: 0.9709\n",
      "Epoch 176/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9876\n",
      "Epoch 176: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0511 - accuracy: 0.9876 - val_loss: 0.2192 - val_accuracy: 0.9477\n",
      "Epoch 177/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9978\n",
      "Epoch 177: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0059 - accuracy: 0.9978 - val_loss: 0.2742 - val_accuracy: 0.9302\n",
      "Epoch 178/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9985\n",
      "Epoch 178: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 0.1841 - val_accuracy: 0.9593\n",
      "Epoch 179/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9993\n",
      "Epoch 179: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.2771 - val_accuracy: 0.9477\n",
      "Epoch 180/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9709\n",
      "Epoch 181/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 8.2704e-04 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 8.2652e-04 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9593\n",
      "Epoch 182/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9869\n",
      "Epoch 182: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0621 - accuracy: 0.9869 - val_loss: 0.2398 - val_accuracy: 0.9535\n",
      "Epoch 183/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 183: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.1766 - val_accuracy: 0.9477\n",
      "Epoch 184/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9985\n",
      "Epoch 184: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0031 - accuracy: 0.9985 - val_loss: 0.1441 - val_accuracy: 0.9767\n",
      "Epoch 185/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1679 - val_accuracy: 0.9709\n",
      "Epoch 186/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 8.5975e-04 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0066 - accuracy: 0.9993 - val_loss: 0.1814 - val_accuracy: 0.9709\n",
      "Epoch 187/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9789\n",
      "Epoch 187: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0785 - accuracy: 0.9789 - val_loss: 0.7108 - val_accuracy: 0.8953\n",
      "Epoch 188/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9985\n",
      "Epoch 188: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.1386 - val_accuracy: 0.9709\n",
      "Epoch 189/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9971\n",
      "Epoch 189: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0075 - accuracy: 0.9971 - val_loss: 0.2202 - val_accuracy: 0.9477\n",
      "Epoch 190/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.2641 - val_accuracy: 0.9593\n",
      "Epoch 191/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9935\n",
      "Epoch 191: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0275 - accuracy: 0.9935 - val_loss: 0.2222 - val_accuracy: 0.9593\n",
      "Epoch 192/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9964\n",
      "Epoch 192: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.2418 - val_accuracy: 0.9651\n",
      "Epoch 193/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9964\n",
      "Epoch 193: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.2266 - val_accuracy: 0.9709\n",
      "Epoch 194/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0462 - accuracy: 0.9862\n",
      "Epoch 194: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0465 - accuracy: 0.9862 - val_loss: 0.4276 - val_accuracy: 0.9012\n",
      "Epoch 195/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 195: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.4467 - val_accuracy: 0.9302\n",
      "Epoch 196/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9964\n",
      "Epoch 196: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0123 - accuracy: 0.9964 - val_loss: 0.2544 - val_accuracy: 0.9477\n",
      "Epoch 197/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 197: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.3792 - val_accuracy: 0.9360\n",
      "Epoch 198/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9956\n",
      "Epoch 198: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.2697 - val_accuracy: 0.9535\n",
      "Epoch 199/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9993    \n",
      "Epoch 199: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0063 - accuracy: 0.9993 - val_loss: 0.2797 - val_accuracy: 0.9477\n",
      "Epoch 200/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0446 - accuracy: 0.9876\n",
      "Epoch 200: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0446 - accuracy: 0.9876 - val_loss: 0.2429 - val_accuracy: 0.9535\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "history=model.fit(\n",
    "    training_set,\n",
    "    epochs=200,\n",
    "    validation_data=validation_set,\n",
    "    batch_size=5,\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AlzqG49vJoAQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model,load_model\n",
    "model=load_model(r\"F:\\JUPYTER NOTEBOOK\\ANIMESH MAITY\\Models\\MobileNetV2_Checkpoints\\MobileNet102-0.9767.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "psn8PMk8JoAR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]], shape=(172, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ar=np.empty(0)\n",
    "for im,y in test_set:\n",
    "    ar=np.append(ar,y)\n",
    "yt=np.zeros((172,5))\n",
    "count=0\n",
    "for i in range(0,172):\n",
    "    for j in range(5):\n",
    "        yt[i][j]=ar[count]\n",
    "        count+=1\n",
    "yt=tf.convert_to_tensor(yt,dtype=tf.float32)\n",
    "print(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "odSm3QFaJoAS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 23s 506ms/step\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "yp=model.predict(test_set)\n",
    "arr=np.zeros(yp.shape)\n",
    "for i in range(yp.shape[0]):\n",
    "    for j in range(yp.shape[1]):\n",
    "        c=yp[i].argmax()\n",
    "        arr[i][c]=1\n",
    "yp=arr\n",
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0RNi77WUJoAT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.9476744186046512\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1 Score= 0.9836065573770492\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(yt,yp)\n",
    "print('Accuracy=',accuracy)\n",
    "precision=precision_score(yt,yp,average=None)\n",
    "print('Precision=',precision[precision.argmax()])\n",
    "recall=recall_score(yt,yp,average=None)\n",
    "print('Recall=',recall[recall.argmax()])\n",
    "f1=f1_score(yt,yp,average=None)\n",
    "print('F1 Score=',f1[f1.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Train acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Test acc')\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Train loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Test loss')\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gjQJ8S64JoAU"
   },
   
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image=tf.keras.utils.load_img(r'F:\\JUPYTER NOTEBOOK\\GROUNDNUT\\dataset\\Test\\Groundnut__Rosette\\Rosette__Test (10).jpg',target_size=(224,224))\n",
    "test_image=tf.keras.utils.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=model.predict(test_image)\n",
    "result=result.flatten()\n",
    "print(result)\n",
    "print(class_names)\n",
    "index=result.argmax()\n",
    "confidence=result[index]*100;\n",
    "pred_class=class_names[index]\n",
    "if pred_class!='Groundnut__Healthy':\n",
    "    print(f'The disease of the given groundnut leaf is {pred_class} predicted with {confidence} % confidence')\n",
    "else:\n",
    "    print(f'The groundnut leaf is healthy predicted with {confidence} % confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLH8KuHyZbdJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
